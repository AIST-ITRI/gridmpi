<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en">
<head>
<link rel="stylesheet" type="text/css" href="man.css">
</head>
<body>

<!-- $Id: install.html,v 1.3 2007/03/01 14:47:24 matu Exp $ -->

<h2>Installation Procedure</h2>

<p>This document describes a step-by-step installation and test
procedure of GridMPI.

<h3>Contents</h3>

<ul>
<li><a href="#pcc">1. Installation on PC Clusters</a>
	<ul>
	<li>1.1. Prerequisite
	<li>1.2. Setting Environment Variables
	<li>1.3. Unpacking the Source
	<li>1.4. Compiling the Source
	<li>1.5. Checking the Installed Files
	<li>1.6. Testing Compilation
	<li>1.7. Starting a Program (as a Cluster MPI)
	<li>1.8. Starting a Program (as a Multiple-Cluster MPI)
	</ul>
<li><a href="#ibmmpi">2. Installation on IBM AIX</a>
	<ul>
	<li>2.1. Prerequisite
	<li>2.2. Setting Environment Variables
	<li>2.3. Unpacking the Source
	<li>2.4. Compiling the Source
	<li>2.5. Checking the Installed Files
	<li>2.6. Testing Compilation
	<li>2.7. Starting a Program (as a Cluster MPI)
	<li>2.8. Starting a Program (as a Multiple-Cluster MPI)
	</ul>
<li><a href="#fjmpi">3. Installation on Fujitsu Solaris/SPARC64V</a>
	<ul>
	<li>3.1. Prerequisite
	<li>3.2. Setting Environment Variables
	<li>3.3. Unpacking the Source
	<li>3.4. Compiling the Source
	<li>3.5. Checking the Installed Files
	<li>3.6. Testing Compilation
	<li>3.7. Starting a Program (as a Cluster MPI)
	<li>3.8. Starting a Program (as a Multiple-Cluster MPI)
	</ul>
<li><a href="#others">4. Notes on Other Platforms</a>
<li><a href="#gridmpi">5. Info: Structure of GridMPI Execution</a>
<li><a href="#gridmpirun">6. Test with GRIDMPIRUN Script</a>
</ul>

<h3>NOTE: Changes from Version 1.0 to Version 1.1</h3>

<p>Version 1.1 is a minor bug fix release.

<ul>
<li>Fix connect failure on Solaris.
<li>Fix Fortran interface in 64bit compilation.
<li>Add a check of pragma-weak support of the C complier.
</ul>

<h3>NOTE: Configuration Templates</h3>

<p>The following lists the recommended options for the configurer.
The configurer shall find default compilers in most cases, and
specifying compilers is optional.  Note that when specifying
<b>--with-binmode=32/64</b> option, do the <b>configure; make; make
install</b> procedure twice, with calling <b>make distclean</b>
between them.  Also, do not mix <b>--with-binmode=no</b> and
<b>--with-binmode=32/64</b> options, which overrides the previously
specified configuration.

<blockquote>
<table border=1 rules=all cellpadding=3>
<tr><th bgcolor=lightgrey>Platform
<th bgcolor=lightgrey>Compiler
<th bgcolor=lightgrey>Configuration
<th bgcolor=lightgrey>Notes
<tr><td>Linux/IA32<td>GCC
	<td>./configure<td>(1)
<tr><td>Linux/IA32<td>Intel
	<td>CC=icc CXX=icpc F77=ifort F90=ifort ./configure<td>(1)
<tr><td>Linux/IA32 (x86_64) (Opteron/EM64T)<td>GCC
	<td>./configure --with-binmode=32<br>
	./configure --with-binmode=64<td>(1)
<tr><td>Linux/IA32 (x86_64) (EM64T)<td>Intel
	<td>CC=icc CXX=icpc F77=ifort F90=ifort ./configure<td>(1)(2)
<tr><td>Linux/IA32 (x86_64) (Opteron/EM64T)<td>PGI
	<td>CC=pgcc CXX=pgCC F77=pgf77 F90=pgf90 ./configure<td>(1)(2)(3)
<tr><td>Linux/IA32 (x86_64) (Opteron/EM64T)<td>Pathscale
	<td>CC=pathcc CXX=pathCC F77=pathf90 F90=pathf90 ./configure
	--with-binmode=32<br>
	CC=pathcc CXX=pathCC F77=pathf90 F90=pathf90 ./configure
	--with-binmode=64<td>(1)
<tr><td>Linux/IA64<td>GCC
	<td>./configure<td>&nbsp;
<tr><td>IBM AIX/Power<td>IBM XL Compilers
	<td>./configure --with-vendormpi=ibmmpi --with-binmode=32<br>
	./configure --with-vendormpi=ibmmpi --with-binmode=64<td>&nbsp;
<tr><td>Hitachi SR11K (AIX/Power)<td>IBM XL and Hitachi F90
	<td>./configure --with-vendormpi=ibmmpi --with-binmode=32<br>
	./configure --with-vendormpi=ibmmpi --with-binmode=64<td>(4)
<tr><td>Fujitsu Solaris8/SPARC64V<td>Fujitsu
	<td>CC=c99 CXX=FCC F77=frt F90=f90 ./configure --with-vendormpi=fjmpi
	--with-binmode=32<br>
	CC=c99 CXX=FCC F77=frt F90=f90 ./configure --with-vendormpi=fjmpi
	--with-binmode=64<td>
<tr><td>Solaris10/SPARC<td>Sun (SUN Studio11)
	<td>./configure --with-binmode=32<br>
	./configure --with-binmode=64<td>&nbsp;
</table>
</blockquote>

<ul>

<li>(1) Checkpointing (libckpt) is enabled by default (and only) on
Linux/IA32 Fedora Core 3 and 4.

<li>(2) Intel and PGI compilers do not support binary mode option.  It
uses different compilers for 32bit and 64bit.  So, do not specify
<b>--with-binmode</b> to the configurer.  It needs to set a proper
path to a needed compiler.

<li>(3) PGI 6.1 or later is needed for compiling code written in ISO
C99 (6.1 is new and we do not have tested it yet).  It may help to add
<b>CFLAGS="-mcmodel=medium"</b> for PGI CC in 64bit mode.  Indices to
data are restricted to 32bits in the default <b>small</b> model in PGI
CC.

<li>(4) The configurer considers <b>f90</b> as Hitachi f90 and sets
the specific options to it, when it finds <b>f90</b> in the path.  It
prefers <b>f90</b> (Hitachi) to <b>xlf90</b> (IBM).

</ul>

<!-- ================================================================ -->
<hr>

<a name="pcc"></a>
<h2>1. Installation on PC Clusters</h2>

<!-- ================================ -->
<h3>1.1. Prerequisite</h3>
<!-- ================================ -->

<p>GridMPI is fully tested with RedHat 9 for IA32 machines with GNU
GCC.  It is checked to compile and run a simplest test on Fedora Core
3 and 4 on IA32, SuSE SLES 8 on x86_64, and SuSE SLES 8 on IA64. Also,
partially tested with Intel Compilers on IA32.

<p>IT IS NEEDED UPDATE GCC 4.0.1 OR LATER FOR FEDORA CORE 4.

<p>GridMPI needs following non-standard commands to compile.

<pre>
	- makedepend
</pre>

<p><b>makedepnd</b> is in the "xorg-x11-devel" RPM package in RedHat
or Fedora Core.

<p>All hosts in clusters need to be global IP address reachable.
[<a href="faq.jsp#ipaddress">FAQ</a>]

<p><i>NOTE: The SCore/PMv2 support (a fast communication library of
the SCore cluster system) is experimental in GridMPI-1.1.  It is
lightly tested only in a cluster environemnt.  It needs further
tuning.</i>

<a name="pcc_env"></a>
<!-- ================================ -->
<h3>1.2. Setting Environment Variables</h3>
<!-- ================================ -->

<p>Set <b>$MPIROOT</b> to the installation directory, and add
<b>$MPIROOT/bin</b> in the PATH.

<p>Commands and libraries are installed and searched in
<b>$MPIROOT/bin</b>, <b>$MPIROOT/include</b> and <b>$MPIROOT/lib</b>.
Note that it does NOT understand shell's "~" notation.  Add settings
in ".profile" or ".cshrc", etc.

<pre class="screen">
# Example assumes /opt/gridmpi as MPIROOT

(For sh/bash)
$ MPIROOT=/opt/gridmpi; export MPIROOT
$ PATH="$MPIROOT/bin:$PATH"; export PATH

(For csh/tcsh)
% setenv MPIROOT /opt/gridmpi
% set path=($MPIROOT/bin $path)
</pre>

<a name="pcc_unpack"></a>
<!-- ================================ -->
<h3>1.3. Unpacking the Source</h3>
<!-- ================================ -->

<p>Unpack the source in an appropriate directory.

<p>In the following, files are expanded under the $HOME directory.
The source expands in the <b>gridmpi-1.1</b> directory.

<pre class="screen">
$ cd $HOME
$ tar zxvf gridmpi-1.1.tar.gz
</pre>

<p>The contents are:

<pre>
	LICENSE:	The NAREGI License
	README:		README file
	checkpoint:	checkpointing source
	configure:	configuration script
	yampii:		YAMPII (PC cluster MPI) source
	src:		GridMPI source
	libpsp:		PSPacer (Precise Software Pacer) support code
</pre>

<a name="pcc_make"></a>
<!-- ================================ -->
<h3>1.4. Compiling the Source</h3>
<!-- ================================ -->

<p>Simply do the following:

<pre class="screen">
$ cd $HOME/gridmpi-1.1			...(1)
$ ./configure				...(2)
$ make					...(3)
$ make install				...(4)
</pre>

<p>(1) Move to the source directory.

<p>(2) Invoke the configurer.  No options suffice for PC cluster
settings.

<p>(3) Make.

<p>(4) Install.

<p>Files are installed in <b>$MPIROOT/bin</b>,
<b>$MPIROOT/include</b>, and <b>$MPIROOT/lib</b>.

<p>See FAQ to use a C complier other than the default one.
[<a href="faq.jsp#config.cc">FAQ</a>]

<!-- ================================ -->
<h3>1.5. Checking the Installed Files</h3>
<!-- ================================ -->

<p>Check the files in the installation directories.

<p>(1) In $MPIROOT/bin,

<pre>
	mpicc, mpif77, mpic++, mpif90, mpirun, gridmpirun,
	impi-server, mpifork, nsd, canquit, detach, gnamesv
</pre>

<p>(2) In $MPIROOT/include,

<pre>
	mpi.h, mpif.h, mpi-1.h, mpi-2.h mpic++.h
</pre>

<p>(3) In $MPIROOT/lib,

<pre>
	libmpi.a
</pre>

<p>(4) Check the commands are in the path.

<pre class="screen">
$ which mpicc
$ which mpif77
$ which mpirun
$ which gridmpirun
</pre>

<a name="pcc_mpicc"></a>
<!-- ================================ -->
<h3>1.6. Testing Compilation</h3>
<!-- ================================ -->

<p>Compile <b>pi.c</b> in the <b>src/test/basic</b> directory.

<p>(1) Compile a test program.

<pre class="screen">
$ cd $HOME/gridmpi-1.1/src/test/basic/
$ mpicc pi.c
</pre>

<a name="pcc_mpirun1"></a>
<!-- ================================ -->
<h3>1.7. Starting a Program (as a Cluster MPI)</h3>
<!-- ================================ -->

<p>(1) Create a configuration file.

<p>Content of <b>mpi_conf</b>:
<blockquote><table border=1 rules=none cellpadding=5><tr><td><pre>
localhost
localhost
</pre></table></blockquote>

<p>(2) Run an application (a.out) as a cluster MPI.

<pre class="screen">
$ mpirun -np 2 ./a.out
</pre>

<p>In this case, GridMPI does not use wide-area communication, and is
a cluster configuration using YAMPII.

<p>When the cluster environment does not support <b>rsh</b>
(remote-shell), it fails because MPI processes are started using
<b>rsh</b> in a cluster by default.  Set the environment variable
<b>_YAMPI_RSH</b> to use <b>ssh</b>.

<pre class="screen">
(For sh/bash)
$ _YAMPI_RSH="ssh -x"; export _YAMPI_RSH

(For csh/tcsh)
% setenv _YAMPI_RSH "ssh -x"
</pre>

<p>Setting of <b>ssh</b> should be with no password.  Refer to the FAQ
to use <b>ssh-agent</b>.  [<a href="faq.jsp#ssh">FAQ</a>]

<a name="pcc_mpirun2"></a>
<!-- ================================ -->
<h3>1.8. Starting a Program (as a Multiple-Cluster MPI)</h3>
<!-- ================================ -->

<p>(1) Create configuration files.  Here, two <b>localhost</b> entires
in <b>mpi_conf1</b>, and two <b>localhost</b> entries in
<b>mpi_conf2</b>.

<p>Content of <b>mpi_conf1</b>:
<blockquote><table border=1 rules=none cellpadding=5><tr><td><pre>
localhost
localhost
</pre></table></blockquote>

<p>Content of <b>mpi_conf2</b>:
<blockquote><table border=1 rules=none cellpadding=5><tr><td><pre>
localhost
localhost
</pre></table></blockquote>

<p>(2) Run an application (a.out).

<pre class="screen">
$ export IMPI_AUTH_NONE=0		...(1)
$ impi-server -server 2 &amp;		...(2)
$ mpirun -client 0 <i>addr:port</i> -np 2 -c mpi_conf1 ./a.out &lt; /dev/null &amp;	...(3)
$ mpirun -client 1 <i>addr:port</i> -np 2 -c mpi_conf2 ./a.out		...(4)
</pre>

<p>(1) Set <b>IMPI_AUTH_NONE</b> environment variable.  It specifies
the authentication method of the impi-server.  The value can be
anything, because it is ignored.

<p>(2) Start the <b>impi-server</b>.  <b>impi-server</b> is a process
to make a contact and to exchange information between MPI processes.
<b>impi-server</b> shall be started each time, because it exits at the
end of an execution of an MPI program.  The <b>-server</b> argument
specifies the number of MPI jobs (invocations of mpirun command).
<b>impi-server</b> prints the IP address/port pair to the stdout.

<p>(3,4) Start MPI jobs by mpirun.  The <b>-client</b> argument
specifies the MPI job ID, and the IP address/port pair of
<b>impi-server</b>.  Job ID is from 0 to the number of jobs minus one
to distinguish mpirun invocations.  The <b>-c</b> option specifies the
list of nodes.  It starts an MPI program with NPROCS=4 (2+2).

<!-- ================================================================ -->
<hr>

<a name="ibmmpi"></a>
<h2>2. Installation on IBM AIX</h2>

<p>GridMPI can use a vendor supplied MPI as an underlying
communication layer as a "Vendor&nbsp;MPI".  It is necessary to
specify options to the configurer to use Vendor&nbsp;MPI.  GridMPI
supports IBM-MPI (on IBM P-Series and Hitachi SR11000) as a
Vendor&nbsp;MPI.

<!-- ================================ -->
<h3>2.1. Prerequisite</h3>
<!-- ================================ -->

<p>GridMPI needs following (non-standard) commands to compile.

<pre>
	- gmake (GNU make)
	- makedepend
	- cc_r and xlc_r
	- IBM-MPI library (assumed to be in /usr/lpp/ppe.poe/lib)
</pre>

<p>GridMPI uses <b>xlc_r</b> to compile the source code.  MPI
applications can be compiled with <b>cc_r</b>, <b>xlf_r</b>, and
Hitachi f90.

<p>When the IBM-MPI library is not installed in the directory
<b>/usr/lpp/ppe.poe/lib</b>, it is necessary to specify its location
by <b>MP_PREFIX</b> (it is needed in both installation and use time).
The <b>MP_PREFIX</b> environment variable is specified by the IBM-MPI.

<!-- ================================ -->
<h3>2.2. Setting Environment Variables</h3>
<!-- ================================ -->

<p>See <b>Installation on PC Clusters.</b>
[<a href="#pcc_env">jump</a>]

<!-- ================================ -->
<h3>2.3. Unpacking the Source</h3>
<!-- ================================ -->

<p>
<p>See <b>Installation on PC Clusters.</b>
[<a href="#pcc_unpack">jump</a>]

<!-- ================================ -->
<h3>2.4. Compiling the Source</h3>
<!-- ================================ -->

<p>The procedure slightly differs from the PC Clusters case in
specifying <b>--with-vendormpi</b> to the configurer and in using
gmake to compile.

<pre class="screen">
$ cd $HOME/gridmpi-1.1			...(1)
$ ./configure --with-vendormpi=ibmmpi --with-binmode=32	...(2)
$ gmake					...(3)
$ gmake install				...(4)
$ gmake distclean
$ ./configure --with-vendormpi=ibmmpi --with-binmode=64	...(2)
$ gmake					...(3)
$ gmake install				...(4)

</pre>

<p>(1) Move to the source directory.

<p>(2) Invoke the configurer.

<p>The <b>--with-vendormpi=ibmmpi</b>
specifies to use Vendor&nbsp;MPI.

<p>The <b>--with-binmode=32/64</b> specifies binary mode.  Use
<b>--with-binmode=no</b> to use a compiler default mode (or when the
compiler does not support options to control the mode).  Use
<b>--with-binmode=32/64</b> to use both modes.  The
<i>configure-make-install</i> procedure shall be performed twice, once
for 32bit mode and once for 64bit mode.  Also specify <b>-q32/-q64</b>
to <b>mpicc</b> at compiling applications.  Do not forget
<b>gmake&nbsp;distclean</b> between two runs of <b>configure</b>.

<p>(3) Make with gmake.

<p>(4) Install.

<p>Files are installed in <b>$MPIROOT/bin</b>,
<b>$MPIROOT/include</b>, and <b>$MPIROOT/lib</b>.

<p>Check the configuration output.  The first configuration
information is from GridMPI.  The second configuration information is
from YAMPII.  Note that GridMPI's configurer calls YAMPII's configurer
inside.  Check that <b>--with-vendormpi</b> is <b>ibmmpi</b>, and
<b>--with-gridmpi</b> is <b>yes</b> in the YAMPII part.

<blockquote><caption><b>Configuration (output from configure)</b></caption>
<table border=1 rules=none cellpadding=5><tr><td><pre>
Configuration
  MPIROOT				/opt/gridmpi
  --enable-debug			no
  --enable-threads			no
  --with-binmode			32/64
  --with-score                          no
  <b>--with-vendormpi			ibmmpi</b>
  --with-libckpt			no
  --with-libpsp				no
  --enable-dlload			yes

Configuration
  MPIROOT				/opt/gridmpi
  --enable-debug			no
  --enable-unix				yes
  --enable-onesided			no
  --enable-threads			no
  --with-binmode			32/64
  --with-score				no
  <b>--with-gridmpi			yes</b>
  <b>--with-vendormpi			ibmmpi</b>
  --with-libckpt			no
  --with-libckpt-includedir		no
  --with-libckpt-libdir			no
  --enable-dlload			yes
</pre></table></blockquote>

<p><b>NOTE</b>: <b>gmake&nbsp;distclean</b> is necessary to clean the
all configuration state when compiling with a different configuration.
Also note that it removes all Makefiles.

<!-- ================================ -->
<h3>2.5. Checking the Installed Files</h3>
<!-- ================================ -->

<p>Check the files in the installation directories.

<p>(1) In $MPIROOT/bin,

<pre>
	mpicc, mpif77, mpic++, mpif90, mpirun, gridmpirun,
	impi-server, mpifork, nsd, canquit, detach, gnamesv
</pre>

<p>(2) In $MPIROOT/include,

<pre>
	mpi.h, mpif.h, mpi-1.h, mpi-2.h mpic++.h
</pre>

<p>(3) In $MPIROOT/lib,

<pre>
	libmpi32.a	(--with-binmode=32 case)
	libmpi64.a	(--with-binmode=64 case)
</pre>

<p>(4) Check the commands are in the path.

<pre class="screen">
$ which mpicc
$ which mpif77
$ which mpirun
$ which gridmpirun
</pre>

<!-- ================================ -->
<h3>2.6. Testing Compilation</h3>
<!-- ================================ -->

<p>Compile <b>pi.c</b> in the <b>src/test/basic</b> directory.

<p>(1) Compile a test program.

<pre class="screen">
$ cd $HOME/gridmpi-1.1/src/test/basic/
$ mpicc -q32 -O3 pi.c (for 32bit binary)
$ mpicc -q64 -O3 pi.c (for 64bit binary)
</pre>

<!-- ================================ -->
<h3>2.7. Starting a Program (as a Cluster MPI)</h3>
<!-- ================================ -->

<p>In the IBM-MPI environment, IBM POE (Parallel Operating
Environment) is used to start MPI processes.  Nodes are specified by a
file <b>host.list</b> in POE by default.  In using POE with the
LoadLeveler, a batch command file <b>llfile</b> is needed.

<p>(1) Create a configuration file.

<p>Content of host.list:
<blockquote><table border=1 rules=none cellpadding=5><tr><td><pre>
node00
node01
</pre></table></blockquote>

<p>Content of llfile:
<blockquote><table border=1 rules=none cellpadding=5><tr><td><pre>
#@job_type=parallel
#@resources=ConsumableCpus(1)
#@queue
</pre></table></blockquote>

<p>(2) Run an application (a.out) as a cluster MPI.

<pre class="screen">
$ mpirun -np 2 ./a.out -llfile llfile
</pre>

<p><b>NOTE</b>: In no LoadLeveler environment, the specification of
<b>-llfile&nbsp;llfile</b> is not necessary.

<p><b>mpirun</b> calls <b>poe</b> command inside to start MPI
processes in the IBM POE.  In the process, the <b>-c</b> argument is
renamed with the <b>-hostfile</b> argument for <b>poe</b> command.

<!-- ================================ -->
<h3>2.8. Starting a Program (as a Multiple-Cluster MPI)</h3>
<!-- ================================ -->

<p>(1) Create configuration files.  Here, two <b>node00</b> entries in
<b>host1.list</b>, and two <b>node01</b> entries in <b>host2.list</b>.

<p>Content of <b>host1.list</b>:
<blockquote><table border=1 rules=none cellpadding=5><tr><td><pre>
node00
node00
</pre></table></blockquote>

<p>Content of <b>host2.list</b>:
<blockquote><table border=1 rules=none cellpadding=5><tr><td><pre>
node01
node01
</pre></table></blockquote>

<p>Content of <b>llfile</b>:
<blockquote><table border=1 rules=none cellpadding=5><tr><td><pre>
#@job_type=parallel
#@resources=ConsumableCpus(1)
#@queue
</pre></table></blockquote>

<p>(2) Run an application (a.out).

<pre class="screen">
$ export IMPI_AUTH_NONE=0
$ impi-server -server 2 &amp;
$ mpirun -client 0 <i>addr:port</i> -np 2 -c host1.list ./a.out -llfile llfile &amp;
$ mpirun -client 1 <i>addr:port</i> -np 2 -c host2.list ./a.out -llfile llfile
</pre>

<p><b>NOTE</b>: In no LoadLeveler environment, the specification of
<b>-llfile&nbsp;llfile</b> is not necessary.

<p>See <b>Installation on PC Clusters</b> for descriptions.
[<a href="#pcc_mpirun2">jump</a>]

<!-- ================================================================ -->
<hr>

<a name="fjmpi"></a>
<h2>3. Installation on Fujitsu Solaris/SPARC64V</h2>

<p>GridMPI supports Fujitsu MPI and Fujitsu compilers in Solaris8
(Fujitsu PrimePower Series).

<!-- ================================ -->
<h3>3.1. Prerequisite</h3>
<!-- ================================ -->

<p>GridMPI needs following (non-standard) commands to compile.

<pre>
	- Fujitsu c99/f90
	- Fujitsu MPI (Parallelnavi)
	- gmake (GNU make)
	- makedepend (in /usr/openwin/bin)
</pre>

<p>The configurer assumes the Fujitsu compilers are installed in
directory <b>/opt/FSUNf90</b>, and the Fujitsu MPI in
<b>/opt/FJSVmpi2</b> and <b>/opt/FSUNaprun</b>.  GridMPI uses Fujitsu
<b>c99</b> to compile the source code.

<!-- ================================ -->
<h3>3.2. Setting Environment Variables</h3>
<!-- ================================ -->

<pre class="screen">
# Example assumes /opt/gridmpi as MPIROOT

(For sh/bash)
$ MPIROOT=/opt/gridmpi; export MPIROOT
$ PATH="$MPIROOT/bin:/opt/FSUNf90/bin:/opt/FSUNaprun/bin:/usr/ccs/bin:$PATH"; export PATH
$ LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/opt/FSUNf90/lib:/opt/FJSVmpi2/lib:\
/opt/FSUNaprun/lib"; export LD_LIBRARY_PATH
$ LD_LIBRARY_PATH_64="$LD_LIBRARY_PATH_64:/opt/FSUNf90/lib/sparcv9:\
/opt/FJSVmpi2/lib/sparcv9:/opt/FSUNaprun/lib/sparcv9:\
/usr/ucblib/sparcv9:/usr/lib/sparcv9"; export LD_LIBRARY_PATH_64

(For csh/tcsh)
% setenv MPIROOT /opt/gridmpi
% set path=($MPIROOT/bin /opt/FSUNf90/bin /opt/FSUNaprun/bin /usr/ccs/bin $path)
% setenv LD_LIBRARY_PATH "${LD_LIBRARY_PATH}:/opt/FSUNf90/lib:/opt/FJSVmpi2/lib:\
/opt/FSUNaprun/lib"
% setenv LD_LIBRARY_PATH_64 "${LD_LIBRARY_PATH_64}:/opt/FSUNf90/lib/sparcv9:\
/opt/FJSVmpi2/lib/sparcv9:/opt/FSUNaprun/lib/sparcv9:\
/usr/ucblib/sparcv9:/usr/lib/sparcv9"
</pre>

<!-- ================================ -->
<h3>3.3. Unpacking the Source</h3>
<!-- ================================ -->

<p>
<p>See <b>Installation on PC Clusters.</b>
[<a href="#pcc_unpack">jump</a>]

<!-- ================================ -->
<h3>3.4. Compiling the Source</h3>
<!-- ================================ -->

<p>The procedure slightly differs from the PC Clusters case in
specifying <b>--with-vendormpi</b> to the configurer and in using
gmake to compile.

<pre class="screen">
$ cd $HOME/gridmpi-1.1			...(1)
$ CC=c99 CXX=FCC F77=frt F90=f90 ./configure --with-vendormpi=fjmpi --with-binmode=32	...(2)
$ gmake					...(3)
$ gmake install				...(4)
$ gmake distclean
$ CC=c99 CXX=FCC F77=frt F90=f90 ./configure --with-vendormpi=fjmpi --with-binmode=64	...(2)
$ gmake					...(3)
$ gmake install				...(4)
</pre>

<p>(1) Move to the source directory.

<p>(2) Invoke the configurer.

<p>The <b>--with-vendormpi=fjmpi</b>
specifies to use Vendor&nbsp;MPI.

<p>The <b>--with-binmode=no/32/64</b> specifies binary mode.  Use
<b>--with-binmode=no</b> to use a compiler default mode (or when the
compiler does not support options to control the mode).  Use
<b>--with-binmode=32/64</b> to use both modes.  The
<i>configure-make-install</i> procedure shall be performed twice, once
for 32bit mode and once for 64bit mode.  Also specify <b>-q32/-q64</b>
to <b>mpicc</b> at compiling applications.  Do not forget
<b>gmake&nbsp;distclean</b> between two runs of <b>configure</b>.

<p>(3) Make with gmake.

<p>(4) Install.

<p>Files are installed in <b>$MPIROOT/bin</b>,
<b>$MPIROOT/include</b>, and <b>$MPIROOT/lib</b>.

<p>Check the configuration output.  The first configuration
information is from GridMPI.  The second configuration information is
from YAMPII.  Note that GridMPI's configurer calls YAMPII's configurer
inside.

<p>Check that <b>--with-vendormpi</b> is <b>fjmpi</b>, and
<b>--with-gridmpi</b> is <b>yes</b> in the YAMPII part.

<blockquote><caption><b>Configuration (output from configure)</b></caption>
<table border=1 rules=none cellpadding=5><tr><td><pre>
Configuration
  MPIROOT				/opt/gridmpi
  --enable-debug			no
  --enable-threads			no
  --with-binmode			32/64
  --with-score				no
  <b>--with-vendormpi			fjmpi</b>
  --with-libckpt			no
  --with-libpsp				no
  --enable-dlload			yes

Configuration
  MPIROOT				/opt/gridmpi
  --enable-debug			no
  --enable-unix				yes
  --enable-onesided			no
  --enable-threads			no
  --with-binmode			32/64
  --with-score				no
  <b>--with-gridmpi			yes</b>
  <b>--with-vendormpi			fjmpi</b>
  --with-libckpt			no
  --with-libckpt-includedir		no
  --with-libckpt-libdir			no
  --enable-dlload			yes
</pre></table></blockquote>

<p><b>NOTE</b>: <b>gmake&nbsp;distclean</b> is necessary to clean the
all configuration state when compiling with a different configuration.
Also note that it removes all Makefiles.

<!-- ================================ -->
<h3>3.5. Checking the Installed Files</h3>
<!-- ================================ -->

<p>Check the files in the installation directories.

<p>(1) In $MPIROOT/bin,

<pre>
	mpicc, mpif77, mpic++, mpif90, mpirun, gridmpirun,
	impi-server, mpifork, nsd, canquit, detach, gnamesv
</pre>

<p>(2) In $MPIROOT/include,

<pre>
	mpi.h, mpif.h, mpi-1.h, mpi-2.h mpic++.h
</pre>

<p>(3) In $MPIROOT/lib,

<pre>
	libmpi32.so libmpi_frt32.a libmpi_gmpi32.so (--with-binmode=32 case)
	libmpi64.so libmpi_frt64.a libmpi_gmpi64.so (--with-binmode=64 case)
</pre>

<p>(4) Check the commands are in the path.

<pre class="screen">
$ which mpicc
$ which mpif77
$ which mpirun
$ which gridmpirun
</pre>

<!-- ================================ -->
<h3>3.6. Testing Compilation</h3>
<!-- ================================ -->

<p>Compile <b>pi.c</b> in the <b>src/test/basic</b> directory.

<p>(1) Compile a test program.

<pre class="screen">
$ cd $HOME/gridmpi-1.1/src/test/basic/
$ mpicc -q32 -Kfast pi.c (for 32bit binary)
$ mpicc -q64 -Kfast -KV9 pi.c (for 64bit binary)
</pre>

<!-- ================================ -->
<h3>3.7. Starting a Program (as a Cluster MPI)</h3>
<!-- ================================ -->

<p>In the Fujitsu MPI environment, MPI runtime
<b>/opt/FJSVmpi2/bin/mpiexec</b> is used to start MPI processes.
Options to <b>mpirun</b> are translated and passed to Fujitsu MPI
mpiexec: <b>-np</b> to <b>-n</b> and <b>-c</b> to <b>-nl</b>.  The
contents of the host configuration file specified by <b>-c</b> is
traslated to the nodelist format of <b>-nl</b>, and the configuration
file should be a list of the host names (a host per line, comments not
allowed).

<p>(1) Run an application (a.out) as a cluster MPI.

<p>Configuration file is not needed.  Fujitsu MPI automatically
configures for available nodes.

<pre class="screen">
$ mpirun -np 2 ./a.out
</pre>

<p><b>mpirun</b> accepts the node list option <b>-nl</b> when
configured with Fujitsu MPI.  For example, only node zero should be
used in the run, pass a list of zeros to the <b>-nl</b> option (one
more zeros is needed to the number to the MPI processes which is
assigned to the daemon process).

<blockquote><table border=1 rules=none cellpadding=5><tr><td><pre>
mpirun -np 4 -nl 0,0,0,0,0
</pre></table></blockquote>

<p><b>mpirun</b> accepts the <b>-c</b> option, which is intended for
using with NODELIST of PBS Pro.

<!-- ================================ -->
<h3>3.8. Starting a Program (as a Multiple-Cluster MPI)</h3>
<!-- ================================ -->

<p>(1) Run an application (a.out).

<pre class="screen">
$ export IMPI_AUTH_NONE=0
$ impi-server -server 2 &amp;
$ mpirun -client 0 <i>addr:port</i> -np 2 ./a.out &amp;
$ mpirun -client 1 <i>addr:port</i> -np 2 ./a.out
</pre>

<!-- ================================================================ -->
<hr>

<a name="others"></a>
<h2>3. Notes on Other Platforms</h2>

<h3>Solaris</h3>

<p>GridMPI is not fully tested in Solaris, but it should work.  It can
be installed and used as PC clusters, except:

<ul>

<li>Use Sun Studio 11 (or later) or GNU GCC.  GridMPI is written
conforming to ISO C99, but earlier versions of Sun Studio only
partially support C99.

<li>Use gmake (GNU make). Makefiles are in the gmake format.

<li>Disable one-sided communication.  GridMPI uses a feature of
changing SIGIO signal number for I/O event notification, but it is not
available in Solaris.  One-sided communication is automatically
disabled.

</ul>

<h3>FreeBSD/IA32</h3>

<p>GridMPI is not fully tested in FreeBSD, but it should work.  It can
be installed and used as PC clusters.

<!-- ================================================================ -->
<hr>

<a name="gridmpi"></a>
<h2>5. Info: Structure of GridMPI Execution</h2>

<p>In running a single cluster MPI, processes are started by
<b>mpirun</b> (without the <b>-client</b> option).  It is just a
cluster MPI, YAMPII, and the YAMPII protocol is used in communication.

<p>In running a multiple-cluster MPI, processes are started by
<b>mpirun -client <i>n</i> <i>ip-address-port</i></b> for each MPI
job.  Multiply invoked MPI jobs join by connecting to an
<b>impi-server</b> process.

<p>The <b>impi-server</b> is a process to exchange information of
processes (e.g., IP address/port pairs of the processes) from multiple
MPI invocations.  It does nothing after exchanging information until
joining processes in MPI_Finalize.

<p>In multiple-cluster MPI, the YAMPII protocol is used for
intra-cluster communication, and the IMPI (Interoperable MPI) protocol
is used for inter-cluster communication.  Multiply started MPI
processes receives their rank with regard to the client number.  The
lowest ranks are assigned to the processes started with <b>mpirun
-client 0</b> and the next lowest are to processes started with
<b>mpirun -client 1</b>, and so on.

<center><table border=1 rules=none cellpadding=5>
<tr><td><pre>
                        IMPI Protocol
          +---------+===================+---------+
          |         |                   |         |
    +-----|---------|-----+       +-----|---------|-----+ 	+--------+
    | +-------+ +-------+ |       | +-------+ +-------+ |	| impi-	 |
    | | rank0 | | rank1 | |       | | rank2 | | rank3 | |	| server |
    | +-------+ +-------+ |       | +-------+ +-------+ |	+--------+
    |     |         |     |       |     |         |     |
    |     +=========+     |       |     +=========+     |
    |   YAMPII Protocol   |       |   YAMPII Protocol   |
    +---------------------+       +---------------------+
       mpirun -client 0		     mpirun -client 1
</pre>
</table></center>

<!-- ================================================================ -->
<hr>

<a name="gridmpirun"></a>
<h2>6. Test with GRIDMPIRUN Script</h2>

<p><b>gridmpirun</b> is a simple frontend script to invoke the
<b>impi-server</b> and a number of mpirun via an rsh/ssh.  It reads
its own configuration file and starts processes as specified.

<p><b>impi_conf</b> specifies the way to start MPI processes on each
cluster.

<p>(1) Create configuration files.  Here, two <b>node00</b> entires in
<b>host1.list</b>, and two <b>node01</b> entries in <b>host2.list</b>.

<p>Content of host1.list:
<blockquote><table border=1 rules=none cellpadding=5><tr><td><pre>
node00
node00
</pre></table></blockquote>

<p>Content of host2.list:
<blockquote><table border=1 rules=none cellpadding=5><tr><td><pre>
node01
node01
</pre></table></blockquote>

<p>Content of <b>impi_conf</b>:
<blockquote><table border=1 rules=none cellpadding=5><tr><td><pre>
-np 2 -c host1.list
-np 2 -c host2.list
</pre></table></blockquote>

<p>Content of <b>llfile</b>:
<blockquote><table border=1 rules=none cellpadding=5><tr><td><pre>
#@job_type=parallel
#@resources=ConsumableCpus(1)
#@queue
</pre></table></blockquote>

<p>(2) Run an application (a.out).

<pre class="screen">
$ gridmpirun -np 4 -machinefile impi_conf ./a.out -llfile llfile
</pre>

<p><b>-np</b> specifies the total number of MPI processes.  gridmpirun
breaks up the number into the sum of the number of processes of each
cluster.

<!-- ================================================================ -->
<hr>
<font size=-2><i>($Date: 2007/03/01 14:47:24 $)</i></font>

</body></html>

<!--
Local Variables:
mode: Fundamental
End:
-->
